Since we're only imputing about 50% of missing IPC values, and when we use a change point detection method, we usually set a jump scale to be 50000, meaning only changes at every 50000 are considered. The problem is, would change point detection be the pure and only reason for good instruction reduction? 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ4NDkxMzIyNV19
-->