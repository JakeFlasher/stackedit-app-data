# Overview
> - Optimized the proposed workflow.
> - Learning from *character.ai*. (quantized attention tuned and spaecialized for very long context length and high industrial throughput) 
> - Workspace tested and pushed to *HPC.docker.io*.

> 7.5.2024
## #1. 
## #2. 
### A glimpse

[Optimizing AI Inference at Character.AI](https://research.character.ai/optimizing-inference/) mentions a highly efficient framework for LLM inference, consisting of the following three parts: 
1.  **Multi-Query Attention (MQA)**: Using a single key for all queries.
2.  **Hybrid Architecture**: Some layers have global attention, while others have only local attention.
3.  **Hierarchical Cache**: Storing the KV cache in memory close to the GPU. During inference, a model determines which KV entries are important and retrieves them to the GPU.

![noam-attention](https://research.character.ai/content/images/2024/06/figure1-2-1.png)
*[Figure 1.](https://research.character.ai/optimizing-inference/) Left: Standard transformer design where every attention is global attention. Right: The attention design in our production model. Blue boxes indicate global attention, green boxes indicate local attention, and curves indicate KV-sharing. For global attention layers, we share KV across multiple non-adjacent layers. This illustration depicts only a subset of the layers in the full model.*
### A detailed look
### A proof from tested results
### A comparison with counterparts
- [StreamingLLM](https://arxiv.org/abs/2309.17453): 
## #3. Workspace tested on *HPC-gz* 
> Due to memory/Hard drive space limit, had to move the current workspace to hpc for faster, parallel executions
- 1. *Dockerhub.io* wasn't working on vm-hpcs, had to download docker image for use from ghrc.io. 
- 2. All tools tested with *SPEC2017* benchmarks (profiling, tracing, simulators etc.)

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTg1MTI5Njc3LDE1MzQ3NzQ2NzUsLTQ5Nz
g4MTcwNCwtMTY2MzA0MDU0OSwtODM1ODMyMTQ1XX0=
-->