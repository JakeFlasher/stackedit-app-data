# Overview
> - Optimized the proposed workflow.
> - Learning from *character.ai*. (quantized attention tuned and spaecialized for very long context length and high industrial throughput) 
> - Workspace tested and pushed to *HPC.docker.io*.

> 7.5.2024
## #1. 
## LLM optimizations from Character.AI
### A glimpse

[Optimizing AI Inference at Character.AI](https://research.character.ai/optimizing-inference/) mentions a highly efficient framework for LLM inference, consisting of the following three parts: 
1.  **Multi-Query Attention (MQA)**: Using a single key for all queries.
![输入图片说明](https://arxiv.org/html/2404.01322v1/extracted/5502412/Attention-Mechanisms.png)
3.  **Hybrid Architecture**: Some layers have global attention, while others have only local attention.
4.  **Hierarchical Cache**: Storing the KV cache in memory close to the GPU. During inference, a model determines which KV entries are important and retrieves them to the GPU.

![noam-attention](https://research.character.ai/content/images/2024/06/figure1-2-1.png)
*[Figure 1.](https://research.character.ai/optimizing-inference/) Left: Standard transformer design where every attention is global attention. Right: The attention design in our production model. Blue boxes indicate global attention, green boxes indicate local attention, and curves indicate KV-sharing. For global attention layers, we share KV across multiple non-adjacent layers. This illustration depicts only a subset of the layers in the full model.*
### A detailed look
### A proof from tested results
### A comparison with counterparts
- [StreamingLLM](https://arxiv.org/abs/2309.17453): 
## Workspace tested on *HPC-gz* 
> Due to memory/Hard drive space limit, had to move the current workspace to hpc for faster, parallel executions
- 1. *Dockerhub.io* wasn't working on vm-hpcs, had to download docker image for use from ghrc.io. 
- 2. All tools tested with *SPEC2017* benchmarks (profiling, tracing, simulators etc.)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEzOTc3NDI5MDIsMTg1MTI5Njc3LDE1Mz
Q3NzQ2NzUsLTQ5Nzg4MTcwNCwtMTY2MzA0MDU0OSwtODM1ODMy
MTQ1XX0=
-->