# Overview
> - Optimized the proposed workflow.
> - Learning from *character.ai*. (quantized attention tuned and spaecialized for very long context length and high industrial throughput) 
> - Workspace tested and pushed to *HPC.docker.io*.

> 7.5.2024
## #1. 
## #2. 
### A glimpse

[Optimizing AI Inference at Character.AI](https://research.character.ai/optimizing-inference/) mentions a highly efficient framework for LLM inference, consisting of the following three parts: 
- mqa;
- shared kv cache;
- local-global mixed attention;
### A detailed look
### A proof from tested results
### A comparison with counterparts
## #3. Workspace tested on *HPC-gz* 
> Due to memory/Hard drive space limit, had to move the current workspace to hpc for faster, parallel executions
- 1. *Dockerhub.io* wasn't working on vm-hpcs, had to download docker image for use from ghrc.io. 
- 2. All tools tested with *SPEC2017* benchmarks (profiling, tracing, simulators etc.)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ5Nzg4MTcwNCwtMTY2MzA0MDU0OSwtOD
M1ODMyMTQ1XX0=
-->