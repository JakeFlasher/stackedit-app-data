# Overview
> - Optimized the proposed workflow.
> - Learning from *character.ai*. (quantized attention tuned and spaecialized for very long context length and high industrial throughput) 
> - Workspace tested and pushed to *HPC.docker.io*.

> 7.5.2024
## #1. 
## LLM optimizations from Character.AI
### A glimpse

[Optimizing AI Inference at Character.AI](https://research.character.ai/optimizing-inference/) mentions a highly efficient framework for LLM inference, consisting of the following three parts: 
1.  **Multi-Query Attention (MQA)**: Using a single key for all queries.
![输入图片说明](https://arxiv.org/html/2404.01322v1/extracted/5502412/Attention-Mechanisms.png)
2.  **Hybrid Attention Horizons**: Some layers have global attention, while others have only local attention.
3.  **Cross Layer KV-sharing***: Storing the KV cache in memory close to the GPU. During inference, a model determines which KV entries are important and retrieves them to the GPU.
![noam-attention](https://research.character.ai/content/images/2024/06/figure1-2-1.png)
*[Figure 1.](https://research.character.ai/optimizing-inference/) Left: Standard transformer design where every attention is global attention. Right: The attention design in our production model. Blue boxes indicate global attention, green boxes indicate local attention, and curves indicate KV-sharing. For global attention layers, we share KV across multiple non-adjacent layers. This illustration depicts only a subset of the layers in the full model.*
4. - **Quantization for Training and Serving**: Implemented customized int8 kernels for  model weights, activations, and attention KV cache. 
>Different from commonly adopted "post-training quantization" techniques, we natively train our models in int8 precision, eliminating the risk of training/serving mismatch while also significantly improving training efficiency. 
### A detailed look
>The key bottleneck of LLM inference throughput is the size of the cache of attention keys and values (KV). It not only determines the maximum batch size that can fit on a GPU, but also dominates the I/O cost on attention layers. We use the following techniques to reduce KV cache size by more than 20X without regressing quality. With these techniques, GPU memory is no longer a bottleneck for serving large batch sizes.
### A proof from tested results
> [Transformer升级之路：9、一种全局长度外推的新思路](https://kexue.fm/archives/9603)
> [Transformer升级之路：14、当HWFA遇见ReRoPE](https://kexue.fm/archives/9731) 
> 
  | 测试长度|512 |4096 |
|----------------|--------------------------------| ------------------------------|
|Baseline|`49.41%` |`24.17%` |
|HFWA|`48.70%` |`80.84%` |  

  | 测试长度|512 |4096 |
|----------------|--------------------------------| ------------------------------|
|Baseline|`49.41%` |`23.17%` |
|HFWA|`48.70%` |`48.15%` |  


### A comparison with counterparts
- [StreamingLLM](https://arxiv.org/abs/2309.17453): 

> We have reduced serving costs by a factor of 33 compared to when we began in late 2022. Today, if we were to serve our traffic using leading commercial APIs, it would cost at least 13.5X more than with our systems.
## Workspace tested on *HPC-gz* 
> Due to memory/Hard drive space limit, had to move the current workspace to hpc for faster, parallel executions
- 1. *Dockerhub.io* wasn't working on vm-hpcs, had to download docker image for use from ghrc.io. 
- 2. All tools tested with *SPEC2017* benchmarks (profiling, tracing, simulators etc.)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ0NDg5Njk5OCwtMTIzMDkwMjMzMiwxNj
E4Nzk1NDIzLC0xMzk3NzQyOTAyLDE4NTEyOTY3NywxNTM0Nzc0
Njc1LC00OTc4ODE3MDQsLTE2NjMwNDA1NDksLTgzNTgzMjE0NV
19
-->