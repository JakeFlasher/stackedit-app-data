# Overview
> - Optimized the proposed workflow.
> - Learning from *character.ai*. (quantized attention tuned and spaecialized for very long context length and high industrial throughput) 
> - Workspace tested and pushed to *HPC.docker.io*.

> 7.5.2024
## #1. 
## LLM optimizations from Character.AI
### A glimpse

[Optimizing AI Inference at Character.AI](https://research.character.ai/optimizing-inference/) mentions a highly efficient framework for LLM inference, consisting of the following three parts: 
1.  **Multi-Query Attention (MQA)**: Using a single key for all queries.
![è¾“å…¥å›¾ç‰‡è¯´æ˜Ž](https://arxiv.org/html/2404.01322v1/extracted/5502412/Attention-Mechanisms.png)
2.  **Hybrid Attention Horizons**: Some layers have global attention, while others have only local attention.
3.  **Cross Layer KV-sharing***: Storing the KV cache in memory close to the GPU. During inference, a model determines which KV entries are important and retrieves them to the GPU.
![noam-attention](https://research.character.ai/content/images/2024/06/figure1-2-1.png)
*[Figure 1.](https://research.character.ai/optimizing-inference/) Left: Standard transformer design where every attention is global attention. Right: The attention design in our production model. Blue boxes indicate global attention, green boxes indicate local attention, and curves indicate KV-sharing. For global attention layers, we share KV across multiple non-adjacent layers. This illustration depicts only a subset of the layers in the full model.*
4. - **Quantization for Training and Serving**: Implemented customized int8 kernels for  model weights, activations, and attention KV cache. 
>Different from commonly adopted "post-training quantization" techniques, we natively train our models in int8 precision, eliminating the risk of training/serving mismatch while also significantly improving training efficiency. 
### A detailed look
>The key bottleneck of LLM inference throughput is the size of the cache of attention keys and values (KV). It not only determines the maximum batch size that can fit on a GPU, but also dominates the I/O cost on attention layers. We use the following techniques to reduce KV cache size by more than 20X without regressing quality. With these techniques, GPU memory is no longer a bottleneck for serving large batch sizes.
### A proof from tested results
> [Transformerå‡çº§ä¹‹è·¯ï¼š9ã€ä¸€ç§å…¨å±€é•¿åº¦å¤–æŽ¨çš„æ–°æ€è·¯](https://kexue.fm/archives/9603)
> [Transformerå‡çº§ä¹‹è·¯ï¼š14ã€å½“HWFAé‡è§ReRoPE](https://kexue.fm/archives/9731) 
> 


> HWFAæ˜¯ä¸€ç§æ³¨æ„åŠ›çš„ç»„åˆæ–¹å¼ï¼Œå®ƒå¯ä»¥ç”¨äºŽæ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œä¹Ÿå¯ä»¥ç”¨äºŽ[GAU](https://kexue.fm/archives/8934)ç­‰æ³¨æ„åŠ›å˜ä½“ä¸­ã€‚ç¬”è€…åœ¨[GAU_alpha](https://kexue.fm/archives/9052)çš„åŸºç¡€ä¸Šè¿›è¡Œäº†å®žéªŒï¼šè®­ç»ƒé•¿åº¦512ï¼Œ24å±‚GAUï¼Œå‰23å±‚ç”¨Window Attentionï¼ŒWindowå¤§å°w=16ð‘¤=16ï¼Œæµ‹è¯•çš„æ˜¯é€tokenå‡†ç¡®çŽ‡ï¼Œå¯¹æ¯”çš„Baselineæ˜¯å…¨éƒ¨å±‚éƒ½æ˜¯Full Attention+RoPEï¼ˆå³å¸¸è§„çš„é»˜è®¤ç”¨æ³•ï¼‰ã€‚


  | æµ‹è¯•é•¿åº¦|512 |4096 |
|----------------|--------------------------------| ------------------------------|
|Baseline|`49.41%` |`24.17%` |
|HFWA|`48.70%` |`80.84%` |  

> 512ä»£è¡¨è®­ç»ƒå‡†ç¡®çŽ‡ï¼ˆä¹Ÿå¯ä»¥å«å†…æ’å‡†ç¡®çŽ‡ï¼‰ï¼Œ4096ä»£è¡¨å¤–æŽ¨å‡†ç¡®çŽ‡ã€‚ä¸ºä»€ä¹ˆè®­ç»ƒå‡†ç¡®çŽ‡æ‰40å¤šï¼Œè€Œå¤–æŽ¨èƒ½åˆ°80å¤šè¿™ä¹ˆå¤¸å¼ ï¼Ÿè¿™æ˜¯å› ä¸ºç¬”è€…åœ¨æž„é€ æµ‹è¯•æ ·æœ¬çš„æ—¶å€™ï¼ŒåŒ…å«äº†éƒ¨åˆ†é‡å¤æ‹¼æŽ¥æ ·æœ¬ï¼Œå³åŒä¸€æ®µä¸è¶…è¿‡4096é•¿åº¦çš„æ–‡æœ¬ï¼Œé€šè¿‡é‡å¤æ‹¼æŽ¥è¾¾åˆ°4096é•¿åº¦ï¼Œç”±äºŽè¿™äº›æ ·æœ¬çš„åŽé¢éƒ¨åˆ†æ˜¯å‰é¢éƒ¨åˆ†çš„é‡å¤ï¼Œå› æ­¤è¿™éƒ¨åˆ†å‡†ç¡®çŽ‡å¾ˆé«˜ï¼ˆå³å‰é¢å·²ç»ç»™å‡ºäº†æ ‡å‡†ç­”æ¡ˆï¼‰ï¼Œè¿™è¯´æ˜Žè·Ÿæˆ‘ä»¬æƒ³è±¡çš„ä¸€æ ·ï¼Œè¿™æ ·çš„è®¾è®¡ä¸‹çš„é•¿åº¦å¤–æŽ¨æ˜¯ä¸ç‰ºç‰²å…¨å±€ä¾èµ–èƒ½åŠ›çš„ã€‚
å¦‚æžœæŠŠé‡å¤æ ·æœ¬å‰”æŽ‰ï¼Œåªä¿ç•™æ­£å¸¸çš„è‡ªç„¶æ–‡æœ¬æ ·æœ¬ï¼Œé‚£ä¹ˆç»“æžœä¹Ÿè¿˜èƒ½çœ‹ï¼š

  | æµ‹è¯•é•¿åº¦|512 |4096 |
|----------------|--------------------------------| ------------------------------|
|Baseline|`49.41%` |`23.17%` |
|HFWA|`48.70%` |`48.15%` |  


### A comparison with counterparts
- [StreamingLLM](https://arxiv.org/abs/2309.17453): 

> We have reduced serving costs by a factor of 33 compared to when we began in late 2022. Today, if we were to serve our traffic using leading commercial APIs, it would cost at least 13.5X more than with our systems.
## Workspace tested on *HPC-gz* 
> Due to memory/Hard drive space limit, had to move the current workspace to hpc for faster, parallel executions
- 1. *Dockerhub.io* wasn't working on vm-hpcs, had to download docker image for use from ghrc.io. 
- 2. All tools tested with *SPEC2017* benchmarks (profiling, tracing, simulators etc.)

<!--stackedit_data:
eyJoaXN0b3J5IjpbMjQ5NTU1NjY1LC00NDQ4OTY5OTgsLTEyMz
A5MDIzMzIsMTYxODc5NTQyMywtMTM5Nzc0MjkwMiwxODUxMjk2
NzcsMTUzNDc3NDY3NSwtNDk3ODgxNzA0LC0xNjYzMDQwNTQ5LC
04MzU4MzIxNDVdfQ==
-->